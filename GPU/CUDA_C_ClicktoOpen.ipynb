{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating C/C++ code with CUDA on GPUs\n",
    "\n",
    "In this self-paced, hands-on lab, we will use CUDA C/C++ to accelerate code on NVIDIA GPUs.\n",
    "\n",
    "Lab created by Mark Ebersole (Follow [@CUDAHamster](https://twitter.com/@cudahamster) on Twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following timer counts down to a five minute warning before the lab instance shuts down.  You should get a pop up at the five minute warning reminding you to save your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script src=\"files/countdown_v5.0/countdown.js\"></script>\n",
    "<div id=\"clock\" align=\"center\"></div>\n",
    "<script>\n",
    "myDate = new Date();\n",
    "curTime = Date.UTC(myDate.getUTCFullYear(), \n",
    "                   myDate.getUTCMonth(), \n",
    "                   myDate.getUTCDate(), \n",
    "                   myDate.getUTCHours(), \n",
    "                   myDate.getUTCMinutes(),\n",
    "                   myDate.getUTCSeconds(),\n",
    "                   myDate.getUTCMilliseconds());\n",
    "\n",
    "function countdownComplete(){\n",
    "  \talert(\"You only have five minutes left in the lab! Time to save your work - see the Post Lab section near the bottom.\");\n",
    "}\n",
    "var myCD = new Countdown({\n",
    "                         time  \t: (1486613312024+110*60000-curTime)/1000,\n",
    "                         target\t \t: \"clock\",\n",
    "                         onComplete\t: countdownComplete,\n",
    "                         rangeHi  : \"minute\",\n",
    "                         hideLine\t: true,\n",
    "                         hideLabels\t: false,\n",
    "                         height\t \t: 60,\n",
    "                         width     : 150,\n",
    "                         style     : \"boring\",\n",
    "                    });\n",
    " </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Before we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see get some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The answer should be three: \" + str(1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<p class=\"hint_trigger\">If you have never before taken an IPython Notebook based self-paced lab from NVIDIA, click this green box.\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">The following video will explain the infrastructure we are using for this self-paced lab, as well as give some tips on it's usage.  If you've never taken a lab on this sytem before, it's highly recommended you watch this short video first.<br><br>\n",
    "<div align=\"center\"><iframe width=\"640\" height=\"390\" src=\"http://www.youtube.com/embed/ZMrDaLSFqpY\" frameborder=\"0\" allowfullscreen></iframe></div>\n",
    "</div></div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CUDA C/C++\n",
    "\n",
    "In this lab, we will learn how to write GPU code using C or C++, two of the ever growing list of CUDA enabled languages.  By learning just a few new syntactic elements, as well as a few API calls, we'll be able to unlock the massively parallel capability of an NVIDIA GPU.\n",
    "\n",
    "Watch the following short video introduction to Grids, Blocks, and Threads:<br><br>\n",
    "\n",
    "<div align=\"center\"><iframe width=\"640\" height=\"390\" src=\"http://www.youtube.com/embed/KM-zbhyz9f4\" frameborder=\"0\" allowfullscreen></iframe></div>\n",
    "\n",
    "This lab consists of three tasks that will require you to modify some code, compile and execute it.  For each task, a solution is provided so you can check your work or take a peek if you get lost.\n",
    "\n",
    "If you are still confused now, or at any point in this lab, you can consult the <a href=\"#FAQ\">FAQ</a> located at the bottom of this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello Parallelism\n",
    "\n",
    "For the first task, we are going to be using the following concepts:\n",
    "\n",
    "* <code style=\"color:green\">&#95;&#95;global&#95;&#95;</code> - this keyword is used to tell the CUDA compiler that the function is to be compiled for the GPU, and is callable from both the host and the GPU itself.  For CUDA C/C++, the nvcc compiler will handle compiling this code.\n",
    "* <code style=\"color:green\">blockIdx.x</code> - this is a read-only variable that is defined for you.  It is used within a GPU kernel to determine the ID of the block which is currently executing code.  Since there will be many blocks running in parallel, we need this ID to help determine which chunk of data that particular block will work on.\n",
    "* <code style=\"color:green\">threadIdx.x</code> - this is a read-only variable that is defined for you.  It is used within a GPU kernel to determine the ID of the thread which is currently executing code in the active block.\n",
    "* <code style=\"color:green\">blockDim.x</code> - this is a read-only variable that is defined for you.  It simply returns a value indicating the number of threads there are per block.  Remember that all the blocks scheduled to execute on the GPU are identical, except for the <code style=\"color:green\">blockIdx.x</code> value.\n",
    "* <code style=\"color:green\">myKernel <<< number_of_blocks, threads_per_block>>> (...)</code> -  this is the syntax used to launch a kernel on the GPU.  Inside the triple-angle brackets we set two values.  The first is the total number of blocks we want to run on the GPU, and the second is the number of threads there are per block.  It's possible, and in fact recommended, for one to schedule more blocks than the GPU can actively run in parallel.  In this case, the system will just continue executing blocks until they have all run.\n",
    "\n",
    "Let's explore the above concepts by doing a simple \"Hello Parallelism\" example.\n",
    "\n",
    "There is nothing you need to do to the code to get this example to work.  Before touching the code at all, select the next cell down and hit Ctrl-Enter (or the play button in the toolbar) to compile using the nvcc compiler from NVIDIA and run it.  The nvcc compiler does the following basic steps:\n",
    "\n",
    "1. From the .cu source file, separate code which should be compiled for the GPU and the code which should be compiled for the CPU\n",
    "2. nvcc will compile the GPU code itself\n",
    "3. nvcc will give the host compiler, in our case gcc, the CPU code to compile\n",
    "4. Link the compiled code from #2 and #3 and create the executable\n",
    "\n",
    "If everything is working, you should see the following: <code>Hello from Thread 0 in block 0</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute this cell to compile & run the Hello Parallelism example labeled task1\n",
    "!nvcc -arch=sm_30 -o task1_out task1/task1.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "Your first task in this lab is to play around with the number of blocks, and threads per block by modifying the values in the `<<< >>>` brackets. To make these changes, click on the `task1` folder in the editor below.  After that you should be able to select task1.cu to open the file.\n",
    "\n",
    "* What do you expect to see if you do `<<<2, 1>>>`?\n",
    "* What do you expect to see if you do `<<<1, 18>>>`? Do the threads print out their values in order?\n",
    "* Can you make the values in the `<<< >>>` arbitrarily large?\n",
    "\n",
    "After making a change, **make sure to save the file** by simply clicking the `save` button below.  As a reminder, saving the file actually saves it on the Amazon GPU system in the cloud you're working on.  To get a copy of the files we'll be working on, consult the <a href=\"#post-lab\">Post-Lab</a> section near the end of this page.\n",
    "\n",
    "To compile and run, simply re-execute the cell above this area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task1\" src=\"task1\" width=\"100%\" height=\"400px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!  You have successfully modified, compiled and executed your first program on the GPU!\n",
    "\n",
    "Besides just getting our hands dirty compiling and executing code on the GPU, task1 was meant to enforce a fundamental principle.  If you set the number of threads per block to at least 18, you should have noticed the threads were no longer printing their Hello Parallelism strings in ascending order - some randomness started to creep in.  The reason for this, is we're executing these blocks & threads on a massively parallel GPU.  So there will be 100's if not 1000's of threads all executing simultaneously with respect to each other.  It's basically a race condition to see who gets to print their values first.\n",
    "\n",
    "In a more realistic example, like we'll work on below, we need to ensure there is enough memory allocated to hold the results from all our threads, so we do not have such a race condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing and Launching GPU Kernels\n",
    "\n",
    "### Task #2\n",
    "\n",
    "Now that you have had some experience launching a function on the GPU with different numbers of threads, it's time to write your first GPU kernel yourself.  You're going to be accelerating the ever popular SAXPY (**S**ingle-precision **A** times **X** **P**lus **Y**) function on the GPU using CUDA C/C++.\n",
    "\n",
    "Using the concepts introduced in Task #1, modify the following code to run on the GPU.  The `#FIXME :...#` text in the code will help you focus on the appropriate areas that need modification.  You'll probably notice two new API calls in the code below; `cudaMallocManaged` and `cudaFree`.  These two functions are working with managed memory using CUDA's Unified Memory system.  We'll explore this in the last task of this lab.  For the moment, you just need to know that they are replacing `malloc` and `free`, respectively.\n",
    "\n",
    "In the text editor below, open the task2.cu file and begin working.  If you get stuck, or just want to check your answer, feel free to look at the task2_solution.cu file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task2\" src=\"task2\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute this cell to compile & run task2.cu\n",
    "!nvcc -arch=sm_30 -o task2_out task2/task2.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of your program should be all 5's.  If you got this, you have successfully modified the saxpy function to execute on the massively parallel GPU!  If you are still not able to get the correct output, please have a look at the task2_solution.cu file and see if you can figure out what you were missing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3\n",
    "\n",
    "The next function we will accelerate is a basic matrix multiplication function.  In this simplified example, we'll assume our matrices are all square - they have the same number of rows and columns.  Your goal is to modify the `matrixMulGPU` function with CUDA so it will run on the GPU.  However, there is a new twist!  Instead of just using one-dimensional blocks of threads and blocks, we'll be using two dimensions; x and y.  So, in addition to using `blockIdx.x`, `blockDim.x`, and `threadIdx.x`, you'll also need to use <code>blockIdx.<span style=\"color:orange\">**y**</span></code>, <code>blockDim.<span style=\"color:orange\">**y**</span></code>, and <code>threadIdx.<span style=\"color:orange\">**y**</span></code>. \n",
    "\n",
    "Besides replacing the `#FIXME: ...#` blocks in the `matrixMulGPU` function, you will need to finish initializing the `number_of_blocks` variable in the `main` function to launch the appropriate number of thread blocks.  This is all marked with `#FIXME: ...#` in the code.\n",
    "\n",
    "Please make use of the hints provided if you get stuck, and you can always check the task3_solution.cu file to see the answer.\n",
    "\n",
    "**Note**: do not modify the CPU version `matrixMulCPU`.  This is used to verify the results of the GPU version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"hint_trigger\">Hint #1\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">You'll be using a single thread to calculate one element of the output matrix `c`.  Each thread will execute the inner-most loop of the matrix multiplication formula.</div></div></div></p>\n",
    "<p class=\"hint_trigger\">Hint #2\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">The `row` value should be set to `blockIdx.x * blockDim.x + threadIdx.x` to get the thread which is calculating the row part of the `c` matrix.  Can you figure out what the `col` value should be set to?</div></div></div></p>\n",
    "<p class=\"hint_trigger\">Hint #3\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">Don't forget the `__global__` keyword to tell the compiler that `matrixMulGPU` is now a GPU function!</div></div></div></p>\n",
    "<p class=\"hint_trigger\">Hint #4\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">Just like the CPU version, we need to make sure we do not do more work than we have data.  Make sure you're doing a check in your modified code so that `row` **and** `col` are less than N.</div></div></div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task3\" src=\"task3\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute this cell to compile & run task3.cu\n",
    "!nvcc -arch=sm_30 -o task3_out task3/task3.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "\n",
    "If you started changing with the number of blocks and threads per block in the above examples, you may have noticed some cases where you were not getting the expected answer.  Up to this point we have not added any type of error checking, which makes it very hard to tell why a problem is occurring.  Error checking is just as important when programming for a GPU as for a CPU.  So let's add some error checking to the SAXPY example and see if we can introduce some errors to catch.\n",
    "\n",
    "**Note**: In order to focus on a specific topic, and to keep code and instructions short, a number of examples in this lab leave out error checking code.  However, it is highly encourage you include error checking in your code whenever possible!\n",
    "\n",
    "### Task #4\n",
    "\n",
    "The SAXPY code for this task is slightly different than in Task #2.  For starters, we are storing the result of our computation back into the `y` array instead of a separate array.  We're also using `float` values for our arrays instead of integers.  On the host side, after calculating our SAXPY result, the code will check for correctness.  If everything is working, the program will print `Max Error: 0.00000`.\n",
    "\n",
    "Now let's discuss the different types of errors we need to check for.  Kernels are launched asynchronously with respect to the host, meaning that control returns to the host code immediately **after** a kernel is launched but **before** the kernel completes. As a result we need to check for two types of error; synchronous errors dealing with the kernel launch and asynchronous errors that can happen during kernel execution. To check for synchronous errors we use the function `cudaGetLastError()`, which returns an integer error code. In the example below we compare the result of `cudaGetLastError()` and if it does not equal `cudaSuccess` (defined in the `cudaFor` module), we have an error:\n",
    "\n",
    "```cpp\n",
    "cudaError_t ierrSync;\n",
    "...\n",
    "myKernel<<<grid, blocks>>>(...);\n",
    "ierrSync = cudaGetLastError();\n",
    "if (ierrSync != cudaSuccess) { printf(\"Sync error: %s\\n\", cudaGetErrorString(ierrSync)); }\n",
    "```\n",
    "\n",
    "To check for an error during execution we need to wait for the kernel to complete.  This is accomplished using the function `cudaDeviceSynchronize()`. As with all CUDA API calls, `cudaDeviceSynchronize()` returns an integer error code and used in this fashion will capture any error that occurs on the device while executing the kernel.  Example usage:\n",
    "\n",
    "```cpp\n",
    "cudaError_t ierrAsync;\n",
    "...\n",
    "myKernel<<<grid, blocks>>>(...);\n",
    "ierrAsync = cudaDeviceSynchronize();\n",
    "if (ierrAsync != cudaSuccess) { printf(\"Async error: %s\\n\", cudaGetErrorString(ierrSync)); }\n",
    "```\n",
    "    \n",
    "In addition to the above code changes, there is a logic bug introduced in the code below.  Your objective for Task #4 is to add both types of error checking to the source code and try to find the bug.  Before modifying any code, compile and run the programs as it is and see the result.  Then add error checking and see if you can figure out what the bug is and fix it.  As always, the solution is provided so you can check your work, or look at if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"hint_trigger\">Hint #1\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">An error string of \"invalid configuration argument\" usually indicates there are to many threads per block being used.</div></div></div></p>\n",
    "<p class=\"hint_trigger\">Hint #2\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">The max number of threads in the X-dimension for our GPU is 1024, but you could choose a value less than that as well.</div></div></div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task4\" src=\"task4\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute this cell to compile & run task4.cu\n",
    "!nvcc -arch=sm_30 -o task4_out task4/task4.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important to consistently add error checking into your code.  We routinely get questions regarding problems with someone's code, only to see there is no error checking.  Almost always once error checking is added, the problem becomes obvious and is easily fixed.  \n",
    "\n",
    "## Querying the GPU Device\n",
    "\n",
    "The CUDA C/C++ device management API allows a programmer to query the number of devices available on a system and the features and capabilities of each device. The simple code below illustrates use of the device management API. After the number of CUDA-enabled devices attached to the system is determined via `cudaGetDeviceCount()`, a loop over these devices is performed (note that devices are enumerated from 0) and the function `cudaGetDeviceProperties()` is used to return information about a device in a variable of type `cudaDeviceProp`. \n",
    "\n",
    "```cpp\n",
    "function deviceQuery\n",
    "{\n",
    "  cudaDeviceProp prop;\n",
    "  int nDevices=0, i, \n",
    "  cudaError_t ierr;\n",
    "\n",
    "  ierr = cudaGetDeviceCount(&nDevices);\n",
    " \n",
    "  for( i = 0, i < nDevices; ++i )\n",
    "  {\n",
    "     ierr = cudaGetDeviceProperties(&prop, i);\n",
    "     printf(\"Device number: %d\\n\", i);\n",
    "     printf(\"  Device name: %s\\n\", prop.name);\n",
    "     printf(\"  Compute capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "     printf(\"  Max threads per block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "     printf(\"  Max threads in X-dimension of block: %d\\n\", prop.maxThreadsDim[0]);\n",
    "     printf(\"  Max threads in Y-dimension of block: %d\\n\", prop.maxThreadsDim[1]);\n",
    "     printf(\"  Max threads in Z-dimension of block: %d\\n\\n\", prop.maxThreadsDim[2]);\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The type `cudaDeviceProp` contains many fields, only a few are demonstrated in this code. One value demonstrated is the compute capability, which indicates the GPU architecture and is given in `major.minor` format. The major component reflects the generation of the architecture, and the minor component the revision within that generation. All Fermi generation GPUs have a compute capability of 2.x,  Kepler GPUs 3.x, and Maxwell GPUs 5.x. In addition to the compute capability, this code prints the maximum number of threads per block possible as well as the maximum number of threads in each dimension of the block.\n",
    "\n",
    "### Task #5\n",
    "\n",
    "For our next task, let's modify a 2D version of the SAXPY code to check for valid launch configuration arguments before launching the kernel.  The launch configuration parameters to check are the number of threads per block and number of blocks, in both the X and Y dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task5\" src=\"task5\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute this cell to compile & run task5.cu\n",
    "!nvcc -arch=sm_30 -o task5_out task5/task5.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try entering different sizes in the block dimension line, `dim3 threads_per_block(32,16,1);`, and make sure your new GPU device property check works correctly!\n",
    "\n",
    "As you begin writing GPU code that could possibly run on multiple or different types of GPUs, you should use the ability to easily query each device to determine the optimal configuration for your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management\n",
    "\n",
    "It is important to realize that the GPU has it's own physical memory; just like the CPU uses system RAM for it's memory.  When executing code on the GPU, we have to ensure any data it needs is first copied across the PCI-Express bus to the GPU's memory before we launch our kernels.\n",
    "\n",
    "Before the release of CUDA 6, it was a requirement that you, the developer, handle allocation and movement of data between the two memory spaces.  This was, and can still be done with the following minimal set of API calls (detailed documentation [here](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY) at [docs.nvidia.com](http://docs.nvidia.com/)):\n",
    "\n",
    "* `cudaMalloc ( void** devPtr, size_t size )` - this API call is used to allocate memory on the GPU, and is very similar to using `malloc` on the CPU.  You provide the addres of a pointer that will point to the memory after the call completes successfully, as well as the number of bytes to allocate.\n",
    "* `cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )` - also very similar the standard `memcpy`, this API call is used to copy data between the CPU and GPU.  It takes a destination pointer, a source pointer, the number of bytes to copy, and the fourth parameter indicates which direction the data is traveling: GPU->CPU, CPU->GPU, or GPU->GPU.\n",
    "* `cudaFree ( void* devPtr )` - we use this API call to free any memory we allocated on the GPU.\n",
    "\n",
    "Starting in CUDA 6, this required manual handling of data movement has been removed with the release of Unified Memory.  Unified Memory creates an area of managed memory, and the underlying system handles moving this managed data between CPU and GPU memory spaces when required.  Because of this, getting an application executing on an NVIDIA GPU with CUDA C/C++ has become a much quicker and efficient process - including simpler to maintain code.\n",
    "\n",
    "To make use of this managed memory area with Unified Memory, you use the following API calls.\n",
    "\n",
    "* `cudaMallocManaged ( T** devPtr, size_t size );` - allocate `size` bytes in managed memory and store in devPtr.\n",
    "* `cudaFree ( void* devPtr )` - we use this API call to free any memory we allocated in managed memory.\n",
    "\n",
    "Once you have used `cudaMallocManaged` to allocate some data, you just use the pointer in your code, regardless of whether it's the CPU or GPU accessing the data.  Before Unified Memory, you typically had two pointers associated with data; one for CPU memory and one for GPU memory (usually using the GPU name preceded with a `d_` to indicate device memory).\n",
    "\n",
    "Managed memory is synchronized between memory spaces at kernel launch and any device synchronization points.  Please visit the CUDA documentation [page on Unified Memory](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd) to read about it in more detail.\n",
    "\n",
    "### Task #6\n",
    "\n",
    "For this task, you need to modify the code in task6.cu so it makes use of Unified Memory instead of the manual data management it currently uses.  If you get stuck, there are a number of hints provided - just click on the green box to see what they are.  After modifying the code, you should get this exact output if you successfully moved to using managed memory.\n",
    "<pre>On device: name=hello, value=10\n",
    "On host: name=dello, value=11\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"hint_trigger\">Hint #1\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">There should be no calls to <code>cudaMemcpy</code> or <code>cudaMalloc</code> in your final solution.</div></div></div></p>\n",
    "<p class=\"hint_trigger\">Hint #2\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">You do not need to modify the <code>Kernel</code> GPU function.</div></div></div></p>\n",
    "<p class=\"hint_trigger\">Hint #3\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">Don't forget that data is synchronized between memory spaces at kernel launch and device synchronization points.  In addition,  launching a kernel from the CPU is asynchronous from the CPU's perspective.  It will continue executing the next line even if the GPU has not completed the kernel.  You will need to add a <code>cudaDeviceSynchronize()</code> call after the kernel launch for the managed memory data synchronization to happen.</div></div></div></p>\n",
    "<p class=\"hint_trigger\">Hint #4\n",
    "      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">Any data that will be shared between the CPU and GPU should be allocated with <code>cudaMallocManaged</code>, not the host-only <code>malloc</code> API.</div></div></div></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task6\" src=\"task6\" width=\"100%\" height=\"600px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute this cell to compile & run task6.cu\n",
    "!nvcc -arch=sm_30 -o task6_out task6/task6.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see why Unified Memory is so appealing - it removes the requirement for complex data management code.  Allowing you to get your functions executing on the GPU with less developer effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study\n",
    "\n",
    "### Task #7\n",
    "\n",
    "The last task in this lab is a case study to practice everything you have learned so far.  Your goal will be to convert the `step_kernel_mod` function to execute on the GPU, as well as modify the `main` function to handle data movement.  It's recommended you first attempt this task with Unified Memory, and then if you are interested, try using `cudaMalloc` and `cudaMemcpy`.\n",
    "\n",
    "In task7.cu, there is a `step_kernel_ref` function that executes on the CPU which will be used for error checking.  This function, and the associated calling code in `main` should not be modified.  The error checking is done at the end of `main` and reports the Max Error between the two results.  Because this code involves floating point calculations, it's important to point out that with the IEEE 754 specification, it's likely that different processors, or even simply reording operations on the same processor, can result in slightly different results.  This is why the error checking code uses an error threshold, instead of looking for an exact match.  You can read about this concept in detail in the [Floating Point and IEEE 754 Compliance for NVIDIA GPUs](http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf) paper written by Nathan Whitehead and Alex Fit-Florea.\n",
    "\n",
    "Credit for the original Heat Conduction CPU source code in this task is given to the article [An OpenACC Example Code for a C-based heat conduction code](http://www.openacc.org/sites/default/files/Heat-Conduction-C--Sample-V2.pdf) from the University of Houston.\n",
    "\n",
    "**NOTE** You may want to proceed to the <a href=\"#post-lab\">Post-Lab</a> section and download this IPython Notebook as well as the source code now, just in case you run out of time before you are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task7\" src=\"task7\" width=\"100%\" height=\"650px\">\n",
    "  <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute this cell to compile & run task7.cu\n",
    "!nvcc -arch=sm_30 -o task7_out task7/task7.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "If you are interested in learning more, you can use the following resources:\n",
    "\n",
    "* Learn more at the [CUDA Developer Zone](https://developer.nvidia.com/category/zone/cuda-zone).\n",
    "* If you have an NVIDIA GPU in your system, you can download and install the [CUDA tookit](https://developer.nvidia.com/cuda-toolkit).\n",
    "* Take the fantastic online and **free** Udacity [Intro to Parallel Programming](https://www.udacity.com/course/cs344) course which uses CUDA C.\n",
    "* Search or ask questions on [Stackoverflow](http://stackoverflow.com/questions/tagged/cuda) using the cuda tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"post-lab\"></a>\n",
    "## Post-Lab\n",
    "\n",
    "Finally, don't forget to save your work from this lab before time runs out and the instance shuts down!!\n",
    "\n",
    "1. Save this IPython Notebook by going to `File -> Download as -> IPython (.ipynb)` at the top of this window\n",
    "2. You can execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f cuda_c_files.zip\n",
    "zip -r cuda_c_files.zip task*/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above cell, you should be able to download the zip file [here](files/cuda_c_files.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"FAQ\"></a>\n",
    "---\n",
    "# Lab FAQ\n",
    "\n",
    "Q: I'm encountering issues executing the cells, or other technical problems?<br>\n",
    "A: Please see [this](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) infrastructure FAQ.\n",
    "\n",
    "Q: I'm getting unexpected behavior (i.e., incorrect output) when running any of the tasks.<br>\n",
    "A: It's possible that one or more of the CUDA Runtime API calls are actually returning an error, but the code above is not checking for this (due to the time constraints of this lab).  Try checking the return value of any CUDA Runtime API call and see if the value is non-zero indicating an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "p.hint_trigger{\n",
    "  margin-bottom:7px;\n",
    "  margin-top:-5px;\n",
    "  background:#64E84D;\n",
    "}\n",
    ".toggle_container{\n",
    "  margin-bottom:0px;\n",
    "}\n",
    ".toggle_container p{\n",
    "  margin:2px;\n",
    "}\n",
    ".toggle_container{\n",
    "  background:#f0f0f0;\n",
    "  clear: both;\n",
    "  font-size:100%;\n",
    "}\n",
    "</style>\n",
    "<script>\n",
    "$(\"p.hint_trigger\").click(function(){\n",
    "   $(this).toggleClass(\"active\").next().slideToggle(\"normal\");\n",
    "});\n",
    "   \n",
    "$(\".toggle_container\").hide();\n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
